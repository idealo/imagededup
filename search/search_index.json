{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Image Deduplicator (imagededup) imagededup is a python package that simplifies the task of finding exact and near duplicates in an image collection. This package provides functionality to make use of hashing algorithms that are particularly good at finding exact duplicates as well as convolutional neural networks which are also adept at finding near duplicates. An evaluation framework is also provided to judge the quality of deduplication for a given dataset. Following details the functionality provided by the package: Finding duplicates in a directory using one of the following algorithms: Convolutional Neural Network (CNN) Perceptual hashing (PHash) Difference hashing (DHash) Wavelet hashing (WHash) Average hashing (AHash) Generation of encodings for images using one of the above stated algorithms. Framework to evaluate effectiveness of deduplication given a ground truth mapping. Plotting duplicates found for a given image file. Detailed documentation for the package can be found at: https://idealo.github.io/imagededup/ imagededup is compatible with Python 3.8+ and runs on Linux, MacOS X and Windows. It is distributed under the Apache 2.0 license. \ud83d\udcd6 Contents Installation Quick Start Benchmarks Contribute Citation Maintainers License \u2699\ufe0f Installation There are two ways to install imagededup: Install imagededup from PyPI (recommended): pip install imagededup Install imagededup from the GitHub source: git clone https://github.com/idealo/imagededup.git cd imagededup pip install \"cython>=0.29\" python setup.py install \ud83d\ude80 Quick Start In order to find duplicates in an image directory using perceptual hashing, following workflow can be used: Import perceptual hashing method from imagededup.methods import PHash phasher = PHash () Generate encodings for all images in an image directory encodings = phasher . encode_images ( image_dir = 'path/to/image/directory' ) Find duplicates using the generated encodings duplicates = phasher . find_duplicates ( encoding_map = encodings ) Plot duplicates obtained for a given file (eg: 'ukbench00120.jpg') using the duplicates dictionary from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , duplicate_map = duplicates , filename = 'ukbench00120.jpg' ) The output looks as below: The complete code for the workflow is: from imagededup.methods import PHash phasher = PHash () # Generate encodings for all images in an image directory encodings = phasher . encode_images ( image_dir = 'path/to/image/directory' ) # Find duplicates using the generated encodings duplicates = phasher . find_duplicates ( encoding_map = encodings ) # plot duplicates obtained for a given file using the duplicates dictionary from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , duplicate_map = duplicates , filename = 'ukbench00120.jpg' ) For more examples, refer this part of the repository. For more detailed usage of the package functionality, refer: https://idealo.github.io/imagededup/ \u23f3 Benchmarks Update : Provided benchmarks are only valid upto imagededup v0.2.2 . The next releases have significant changes to all methods, so the current benchmarks may not hold. Detailed benchmarks on speed and classification metrics for different methods have been provided in the documentation . Generally speaking, following conclusions can be made: CNN works best for near duplicates and datasets containing transformations. All deduplication methods fare well on datasets containing exact duplicates, but Difference hashing is the fastest. \ud83e\udd1d Contribute We welcome all kinds of contributions. See the Contribution guide for more details. \ud83d\udcdd Citation Please cite Imagededup in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { idealods2019imagededup , title = {Imagededup} , author = {Tanuj Jain and Christopher Lennan and Zubin John and Dat Tran} , year = {2019} , howpublished = {\\url{https://github.com/idealo/imagededup}} , } \ud83c\udfd7 Maintainers Tanuj Jain, github: tanujjain Christopher Lennan, github: clennan Dat Tran, github: datitran \u00a9 Copyright See LICENSE for details.","title":"Home"},{"location":"#image-deduplicator-imagededup","text":"imagededup is a python package that simplifies the task of finding exact and near duplicates in an image collection. This package provides functionality to make use of hashing algorithms that are particularly good at finding exact duplicates as well as convolutional neural networks which are also adept at finding near duplicates. An evaluation framework is also provided to judge the quality of deduplication for a given dataset. Following details the functionality provided by the package: Finding duplicates in a directory using one of the following algorithms: Convolutional Neural Network (CNN) Perceptual hashing (PHash) Difference hashing (DHash) Wavelet hashing (WHash) Average hashing (AHash) Generation of encodings for images using one of the above stated algorithms. Framework to evaluate effectiveness of deduplication given a ground truth mapping. Plotting duplicates found for a given image file. Detailed documentation for the package can be found at: https://idealo.github.io/imagededup/ imagededup is compatible with Python 3.8+ and runs on Linux, MacOS X and Windows. It is distributed under the Apache 2.0 license.","title":"Image Deduplicator (imagededup)"},{"location":"#contents","text":"Installation Quick Start Benchmarks Contribute Citation Maintainers License","title":"\ud83d\udcd6 Contents"},{"location":"#installation","text":"There are two ways to install imagededup: Install imagededup from PyPI (recommended): pip install imagededup Install imagededup from the GitHub source: git clone https://github.com/idealo/imagededup.git cd imagededup pip install \"cython>=0.29\" python setup.py install","title":"\u2699\ufe0f Installation"},{"location":"#quick-start","text":"In order to find duplicates in an image directory using perceptual hashing, following workflow can be used: Import perceptual hashing method from imagededup.methods import PHash phasher = PHash () Generate encodings for all images in an image directory encodings = phasher . encode_images ( image_dir = 'path/to/image/directory' ) Find duplicates using the generated encodings duplicates = phasher . find_duplicates ( encoding_map = encodings ) Plot duplicates obtained for a given file (eg: 'ukbench00120.jpg') using the duplicates dictionary from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , duplicate_map = duplicates , filename = 'ukbench00120.jpg' ) The output looks as below: The complete code for the workflow is: from imagededup.methods import PHash phasher = PHash () # Generate encodings for all images in an image directory encodings = phasher . encode_images ( image_dir = 'path/to/image/directory' ) # Find duplicates using the generated encodings duplicates = phasher . find_duplicates ( encoding_map = encodings ) # plot duplicates obtained for a given file using the duplicates dictionary from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , duplicate_map = duplicates , filename = 'ukbench00120.jpg' ) For more examples, refer this part of the repository. For more detailed usage of the package functionality, refer: https://idealo.github.io/imagededup/","title":"\ud83d\ude80 Quick Start"},{"location":"#benchmarks","text":"Update : Provided benchmarks are only valid upto imagededup v0.2.2 . The next releases have significant changes to all methods, so the current benchmarks may not hold. Detailed benchmarks on speed and classification metrics for different methods have been provided in the documentation . Generally speaking, following conclusions can be made: CNN works best for near duplicates and datasets containing transformations. All deduplication methods fare well on datasets containing exact duplicates, but Difference hashing is the fastest.","title":"\u23f3 Benchmarks"},{"location":"#contribute","text":"We welcome all kinds of contributions. See the Contribution guide for more details.","title":"\ud83e\udd1d Contribute"},{"location":"#citation","text":"Please cite Imagededup in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { idealods2019imagededup , title = {Imagededup} , author = {Tanuj Jain and Christopher Lennan and Zubin John and Dat Tran} , year = {2019} , howpublished = {\\url{https://github.com/idealo/imagededup}} , }","title":"\ud83d\udcdd Citation"},{"location":"#maintainers","text":"Tanuj Jain, github: tanujjain Christopher Lennan, github: clennan Dat Tran, github: datitran","title":"\ud83c\udfd7 Maintainers"},{"location":"#copyright","text":"See LICENSE for details.","title":"\u00a9 Copyright"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it is: Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue on GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and create a new branch from the dev branch. For bug fixes, add new tests and for new features, please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Imagededup repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it is: Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue on GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and create a new branch from the dev branch. For bug fixes, add new tests and for new features, please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Imagededup repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"Copyright 2019 idealo internet GmbH. All rights reserved. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work , attach the following boilerplate notice , with the fields enclosed by brackets \"[]\" replaced with your own identifying information . ( Don 't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"evaluation/evaluation/","text":"evaluate def evaluate ( ground_truth_map , retrieved_map , metric ) Given a ground truth map and a duplicate map retrieved from a deduplication algorithm, get metrics to evaluate the effectiveness of the applied deduplication algorithm. Args ground_truth_map : A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric : Name of metric to be evaluated and returned. Accepted values are: 'map', 'ndcg', 'jaccard', 'classification', 'all'(default, returns every metric). Returns dictionary : A dictionary with metric name as key and corresponding calculated metric as the value. 'map', 'ndcg' and 'jaccard' return a single number denoting the corresponding information retrieval metric. 'classification' metrics include 'precision', 'recall' and 'f1-score' which are returned in the form of individual entries in the returned dictionary. The value for each of the classification metric is a numpy array with first entry as the score for non-duplicate file pairs(class-0) and second entry as the score for duplicate file pairs (class-1). Additionally, a support is also returned as another key with first entry denoting number of non-duplicate file pairs and second entry having duplicate file pairs.","title":"Evaluation"},{"location":"evaluation/evaluation/#evaluate","text":"def evaluate ( ground_truth_map , retrieved_map , metric ) Given a ground truth map and a duplicate map retrieved from a deduplication algorithm, get metrics to evaluate the effectiveness of the applied deduplication algorithm.","title":"evaluate"},{"location":"evaluation/evaluation/#args","text":"ground_truth_map : A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric : Name of metric to be evaluated and returned. Accepted values are: 'map', 'ndcg', 'jaccard', 'classification', 'all'(default, returns every metric).","title":"Args"},{"location":"evaluation/evaluation/#returns","text":"dictionary : A dictionary with metric name as key and corresponding calculated metric as the value. 'map', 'ndcg' and 'jaccard' return a single number denoting the corresponding information retrieval metric. 'classification' metrics include 'precision', 'recall' and 'f1-score' which are returned in the form of individual entries in the returned dictionary. The value for each of the classification metric is a numpy array with first entry as the score for non-duplicate file pairs(class-0) and second entry as the score for duplicate file pairs (class-1). Additionally, a support is also returned as another key with first entry denoting number of non-duplicate file pairs and second entry having duplicate file pairs.","title":"Returns"},{"location":"examples/CIFAR10_deduplication/","text":"CIFAR10 deduplication example Install imagededup via PyPI !pip install imagededup Download CIFAR10 dataset and untar !wget http://pjreddie.com/media/files/cifar.tgz !tar xzf cifar.tgz Create working directory and move all images into this directory image_dir = 'cifar10_images' !mkdir $image_dir !cp -r '/content/cifar/train/.' $image_dir !cp -r '/content/cifar/test/.' $image_dir Find duplicates in the entire dataset with CNN from imagededup.methods import CNN cnn = CNN () encodings = cnn . encode_images ( image_dir = image_dir ) duplicates = cnn . find_duplicates ( encoding_map = encodings ) Do some imports for plotting from pathlib import Path from imagededup.utils import plot_duplicates import matplotlib.pyplot as plt plt . rcParams [ 'figure.figsize' ] = ( 15 , 10 ) Find and plot duplicates in the test set with CNN # test images are stored under '/content/cifar/test' filenames_test = set ([ i . name for i in Path ( '/content/cifar/test' ) . glob ( '*.png' )]) duplicates_test = {} for k , v in duplicates . items (): if k in filenames_test : tmp = [ i for i in v if i in filenames_test ] duplicates_test [ k ] = tmp # sort in descending order of duplicates duplicates_test = { k : v for k , v in sorted ( duplicates_test . items (), key = lambda x : len ( x [ 1 ]), reverse = True )} # plot duplicates found for some file plot_duplicates ( image_dir = image_dir , duplicate_map = duplicates_test , filename = list ( duplicates_test . keys ())[ 0 ]) Find and plot duplicates in the train set with CNN # train images are stored under '/content/cifar/train' filenames_train = set ([ i . name for i in Path ( '/content/cifar/train' ) . glob ( '*.png' )]) duplicates_train = {} for k , v in duplicates . items (): if k in filenames_train : tmp = [ i for i in v if i in filenames_train ] duplicates_train [ k ] = tmp # sort in descending order of duplicates duplicates_train = { k : v for k , v in sorted ( duplicates_train . items (), key = lambda x : len ( x [ 1 ]), reverse = True )} # plot duplicates found for some file plot_duplicates ( image_dir = image_dir , duplicate_map = duplicates_train , filename = list ( duplicates_train . keys ())[ 0 ]) Examples from test set with duplicates in train set # keep only filenames that are in test set have duplicates in train set duplicates_test_train = {} for k , v in duplicates . items (): if k in filenames_test : tmp = [ i for i in v if i in filenames_train ] duplicates_test_train [ k ] = tmp # sort in descending order of duplicates duplicates_test_train = { k : v for k , v in sorted ( duplicates_test_train . items (), key = lambda x : len ( x [ 1 ]), reverse = True )} # plot duplicates found for some file plot_duplicates ( image_dir = image_dir , duplicate_map = duplicates_test_train , filename = list ( duplicates_test_train . keys ())[ 0 ])","title":"CIFAR10 deduplication"},{"location":"examples/CIFAR10_deduplication/#cifar10-deduplication-example","text":"","title":"CIFAR10 deduplication example"},{"location":"examples/CIFAR10_deduplication/#install-imagededup-via-pypi","text":"!pip install imagededup","title":"Install imagededup via PyPI"},{"location":"examples/CIFAR10_deduplication/#download-cifar10-dataset-and-untar","text":"!wget http://pjreddie.com/media/files/cifar.tgz !tar xzf cifar.tgz","title":"Download CIFAR10 dataset and untar"},{"location":"examples/CIFAR10_deduplication/#create-working-directory-and-move-all-images-into-this-directory","text":"image_dir = 'cifar10_images' !mkdir $image_dir !cp -r '/content/cifar/train/.' $image_dir !cp -r '/content/cifar/test/.' $image_dir","title":"Create working directory and move all images into this directory"},{"location":"examples/CIFAR10_deduplication/#find-duplicates-in-the-entire-dataset-with-cnn","text":"from imagededup.methods import CNN cnn = CNN () encodings = cnn . encode_images ( image_dir = image_dir ) duplicates = cnn . find_duplicates ( encoding_map = encodings )","title":"Find duplicates in the entire dataset with CNN"},{"location":"examples/CIFAR10_deduplication/#do-some-imports-for-plotting","text":"from pathlib import Path from imagededup.utils import plot_duplicates import matplotlib.pyplot as plt plt . rcParams [ 'figure.figsize' ] = ( 15 , 10 )","title":"Do some imports for plotting"},{"location":"examples/CIFAR10_deduplication/#find-and-plot-duplicates-in-the-test-set-with-cnn","text":"# test images are stored under '/content/cifar/test' filenames_test = set ([ i . name for i in Path ( '/content/cifar/test' ) . glob ( '*.png' )]) duplicates_test = {} for k , v in duplicates . items (): if k in filenames_test : tmp = [ i for i in v if i in filenames_test ] duplicates_test [ k ] = tmp # sort in descending order of duplicates duplicates_test = { k : v for k , v in sorted ( duplicates_test . items (), key = lambda x : len ( x [ 1 ]), reverse = True )} # plot duplicates found for some file plot_duplicates ( image_dir = image_dir , duplicate_map = duplicates_test , filename = list ( duplicates_test . keys ())[ 0 ])","title":"Find and plot duplicates in the test set with CNN"},{"location":"examples/CIFAR10_deduplication/#find-and-plot-duplicates-in-the-train-set-with-cnn","text":"# train images are stored under '/content/cifar/train' filenames_train = set ([ i . name for i in Path ( '/content/cifar/train' ) . glob ( '*.png' )]) duplicates_train = {} for k , v in duplicates . items (): if k in filenames_train : tmp = [ i for i in v if i in filenames_train ] duplicates_train [ k ] = tmp # sort in descending order of duplicates duplicates_train = { k : v for k , v in sorted ( duplicates_train . items (), key = lambda x : len ( x [ 1 ]), reverse = True )} # plot duplicates found for some file plot_duplicates ( image_dir = image_dir , duplicate_map = duplicates_train , filename = list ( duplicates_train . keys ())[ 0 ])","title":"Find and plot duplicates in the train set with CNN"},{"location":"examples/CIFAR10_deduplication/#examples-from-test-set-with-duplicates-in-train-set","text":"# keep only filenames that are in test set have duplicates in train set duplicates_test_train = {} for k , v in duplicates . items (): if k in filenames_test : tmp = [ i for i in v if i in filenames_train ] duplicates_test_train [ k ] = tmp # sort in descending order of duplicates duplicates_test_train = { k : v for k , v in sorted ( duplicates_test_train . items (), key = lambda x : len ( x [ 1 ]), reverse = True )} # plot duplicates found for some file plot_duplicates ( image_dir = image_dir , duplicate_map = duplicates_test_train , filename = list ( duplicates_test_train . keys ())[ 0 ])","title":"Examples from test set with duplicates in train set"},{"location":"handlers/metrics/classification/","text":"classification_metrics def classification_metrics ( ground_truth , retrieved ) Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is assigned to duplicate file pairs while class 0 is for non-duplicate file pairs. Args ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved Returns","title":"Classification"},{"location":"handlers/metrics/classification/#classification_metrics","text":"def classification_metrics ( ground_truth , retrieved ) Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.","title":"classification_metrics"},{"location":"handlers/metrics/classification/#args","text":"ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved","title":"Args"},{"location":"handlers/metrics/classification/#returns","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/","text":"avg_prec def avg_prec ( correct_duplicates , retrieved_duplicates ) Get average precision(AP) for a single query given correct and retrieved file names. Args correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query Returns ndcg def ndcg ( correct_duplicates , retrieved_duplicates ) Get Normalized discounted cumulative gain(NDCG) for a single query given correct and retrieved file names. Args correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query Returns jaccard_similarity def jaccard_similarity ( correct_duplicates , retrieved_duplicates ) Get jaccard similarity for a single query given correct and retrieved file names. Args correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query Returns mean_metric def mean_metric ( ground_truth , retrieved , metric ) Get mean of specified metric. Args metric_func : metric function on which mean is to be calculated across all queries Returns get_all_metrics def get_all_metrics ( ground_truth , retrieved ) Get mean of all information retrieval metrics across all queries. Args ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved Returns","title":"Information retrieval"},{"location":"handlers/metrics/information_retrieval/#avg_prec","text":"def avg_prec ( correct_duplicates , retrieved_duplicates ) Get average precision(AP) for a single query given correct and retrieved file names.","title":"avg_prec"},{"location":"handlers/metrics/information_retrieval/#args","text":"correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#ndcg","text":"def ndcg ( correct_duplicates , retrieved_duplicates ) Get Normalized discounted cumulative gain(NDCG) for a single query given correct and retrieved file names.","title":"ndcg"},{"location":"handlers/metrics/information_retrieval/#args_1","text":"correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_1","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#jaccard_similarity","text":"def jaccard_similarity ( correct_duplicates , retrieved_duplicates ) Get jaccard similarity for a single query given correct and retrieved file names.","title":"jaccard_similarity"},{"location":"handlers/metrics/information_retrieval/#args_2","text":"correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_2","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#mean_metric","text":"def mean_metric ( ground_truth , retrieved , metric ) Get mean of specified metric.","title":"mean_metric"},{"location":"handlers/metrics/information_retrieval/#args_3","text":"metric_func : metric function on which mean is to be calculated across all queries","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_3","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#get_all_metrics","text":"def get_all_metrics ( ground_truth , retrieved ) Get mean of all information retrieval metrics across all queries.","title":"get_all_metrics"},{"location":"handlers/metrics/information_retrieval/#args_4","text":"ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_4","text":"","title":"Returns"},{"location":"handlers/search/bktree/","text":"class BkTreeNode Class to contain the attributes of a single node in the BKTree. __init__ def __init__ ( node_name , node_value , parent_name ) class BKTree Class to construct and perform search using a BKTree. __init__ def __init__ ( hash_dict , distance_function ) Initialize a root for the BKTree and triggers the tree construction using the dictionary for mapping file names and corresponding hashes. Args hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes. construct_tree def construct_tree () Construct the BKTree. search def search ( query , tol ) Function to search the bktree given a hash of the query image. Args query : hash string for which BKTree needs to be searched. tol : distance upto which duplicate is valid. Returns List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Bktree"},{"location":"handlers/search/bktree/#class-bktreenode","text":"Class to contain the attributes of a single node in the BKTree.","title":"class BkTreeNode"},{"location":"handlers/search/bktree/#__init__","text":"def __init__ ( node_name , node_value , parent_name )","title":"__init__"},{"location":"handlers/search/bktree/#class-bktree","text":"Class to construct and perform search using a BKTree.","title":"class BKTree"},{"location":"handlers/search/bktree/#__init___1","text":"def __init__ ( hash_dict , distance_function ) Initialize a root for the BKTree and triggers the tree construction using the dictionary for mapping file names and corresponding hashes.","title":"__init__"},{"location":"handlers/search/bktree/#args","text":"hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes.","title":"Args"},{"location":"handlers/search/bktree/#construct_tree","text":"def construct_tree () Construct the BKTree.","title":"construct_tree"},{"location":"handlers/search/bktree/#search","text":"def search ( query , tol ) Function to search the bktree given a hash of the query image.","title":"search"},{"location":"handlers/search/bktree/#args_1","text":"query : hash string for which BKTree needs to be searched. tol : distance upto which duplicate is valid.","title":"Args"},{"location":"handlers/search/bktree/#returns","text":"List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Returns"},{"location":"handlers/search/brute_force/","text":"class BruteForce Class to perform search using a Brute force. __init__ def __init__ ( hash_dict , distance_function ) Initialize a dictionary for mapping file names and corresponding hashes and a distance function to be used for getting distance between two hash strings. Args hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes. search def search ( query , tol ) Function for searching using brute force. Args query : hash string for which brute force needs to work. tol : distance upto which duplicate is valid. Returns List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Brute force"},{"location":"handlers/search/brute_force/#class-bruteforce","text":"Class to perform search using a Brute force.","title":"class BruteForce"},{"location":"handlers/search/brute_force/#__init__","text":"def __init__ ( hash_dict , distance_function ) Initialize a dictionary for mapping file names and corresponding hashes and a distance function to be used for getting distance between two hash strings.","title":"__init__"},{"location":"handlers/search/brute_force/#args","text":"hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes.","title":"Args"},{"location":"handlers/search/brute_force/#search","text":"def search ( query , tol ) Function for searching using brute force.","title":"search"},{"location":"handlers/search/brute_force/#args_1","text":"query : hash string for which brute force needs to work. tol : distance upto which duplicate is valid.","title":"Args"},{"location":"handlers/search/brute_force/#returns","text":"List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Returns"},{"location":"handlers/search/brute_force_cython/","text":"class BruteForceCython Class to perform search using a Brute force. __init__ def __init__ ( hash_dict , distance_function ) Initialize a dictionary for mapping file names and corresponding hashes and a distance function to be used for getting distance between two hash strings. Args hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes. search def search ( query , tol ) Function for searching using brute force. Args query : hash string for which brute force needs to work. tol : distance upto which duplicate is valid. Returns List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Brute force cython"},{"location":"handlers/search/brute_force_cython/#class-bruteforcecython","text":"Class to perform search using a Brute force.","title":"class BruteForceCython"},{"location":"handlers/search/brute_force_cython/#__init__","text":"def __init__ ( hash_dict , distance_function ) Initialize a dictionary for mapping file names and corresponding hashes and a distance function to be used for getting distance between two hash strings.","title":"__init__"},{"location":"handlers/search/brute_force_cython/#args","text":"hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes.","title":"Args"},{"location":"handlers/search/brute_force_cython/#search","text":"def search ( query , tol ) Function for searching using brute force.","title":"search"},{"location":"handlers/search/brute_force_cython/#args_1","text":"query : hash string for which brute force needs to work. tol : distance upto which duplicate is valid.","title":"Args"},{"location":"handlers/search/brute_force_cython/#returns","text":"List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Returns"},{"location":"handlers/search/retrieval/","text":"cosine_similarity_chunk def cosine_similarity_chunk ( t ) get_cosine_similarity def get_cosine_similarity ( X , verbose , chunk_size , threshold ) class HashEval __init__ def __init__ ( test , queries , distance_function , verbose , threshold , search_method ) Initialize a HashEval object which offers an interface to control hashing and search methods for desired dataset. Compute a map of duplicate images in the document space given certain input control parameters. retrieve_results def retrieve_results ( scores ) Return results with or without scores. Args scores : Boolean indicating whether results are to eb returned with or without scores. Returns if scores is True, then a dictionary of the form {'image1.jpg' : [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg' : [] ..} if scores is False, then a dictionary of the form {'image1.jpg' : ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg' : ['image1_duplicate1.jpg',..], ..}","title":"Retrieval"},{"location":"handlers/search/retrieval/#cosine_similarity_chunk","text":"def cosine_similarity_chunk ( t )","title":"cosine_similarity_chunk"},{"location":"handlers/search/retrieval/#get_cosine_similarity","text":"def get_cosine_similarity ( X , verbose , chunk_size , threshold )","title":"get_cosine_similarity"},{"location":"handlers/search/retrieval/#class-hasheval","text":"","title":"class HashEval"},{"location":"handlers/search/retrieval/#__init__","text":"def __init__ ( test , queries , distance_function , verbose , threshold , search_method ) Initialize a HashEval object which offers an interface to control hashing and search methods for desired dataset. Compute a map of duplicate images in the document space given certain input control parameters.","title":"__init__"},{"location":"handlers/search/retrieval/#retrieve_results","text":"def retrieve_results ( scores ) Return results with or without scores.","title":"retrieve_results"},{"location":"handlers/search/retrieval/#args","text":"scores : Boolean indicating whether results are to eb returned with or without scores.","title":"Args"},{"location":"handlers/search/retrieval/#returns","text":"if scores is True, then a dictionary of the form {'image1.jpg' : [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg' : [] ..} if scores is False, then a dictionary of the form {'image1.jpg' : ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg' : ['image1_duplicate1.jpg',..], ..}","title":"Returns"},{"location":"methods/cnn/","text":"class CNN Find duplicates using CNN and/or generate CNN encodings given a single image or a directory of images. The module can be used for 2 purposes: Encoding generation and duplicate detection. Encodings generation: To propagate an image through a Convolutional Neural Network architecture and generate encodings. The generated encodings can be used at a later time for deduplication. Using the method 'encode_image', the CNN encodings for a single image can be obtained while the 'encode_images' method can be used to get encodings for all images in a directory. Duplicate detection: Find duplicates either using the encoding mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplciates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks. __init__ def __init__ ( verbose ) Initialize a pytorch MobileNet model v3 that is sliced at the last convolutional layer. Set the batch size for pytorch dataloader to be 64 samples. Args verbose : Display progress bar if True else disable it. Default value is True. apply_mobilenet_preprocess def apply_mobilenet_preprocess ( im_arr ) encode_image def encode_image ( image_file , image_array ) Generate CNN encoding for a single image. Args image_file : Path to the image file. image_array : Optional, used instead of image_file. Image typecast to numpy array. Returns encoding : Encodings for the image in the form of numpy array. Example usage from imagededup.methods import CNN myencoder = CNN () encoding = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR encoding = myencoder . encode_image ( image_array =< numpy array of image > ) encode_images def encode_images ( image_dir , recursive ) Generate CNN encodings for all images in a given directory of images. Test. Args image_dir : Path to the image directory. recursive : Optional, find images recursively in the image directory. Returns dictionary : Contains a mapping of filenames and corresponding numpy array of CNN encodings. Example usage from imagededup.methods import CNN myencoder = CNN () encoding_map = myencoder . encode_images ( image_dir = 'path/to/image/directory' ) find_duplicates def find_duplicates ( image_dir , encoding_map , min_similarity_threshold , scores , outfile , recursive ) Find duplicates for each file. Take in path of the directory or encoding dictionary in which duplicates are to be detected above the given threshold. Return dictionary containing key as filename and value as a list of duplicate file names. Optionally, the cosine distances could be returned instead of just duplicate filenames for each query file. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding CNN encodings. min_similarity_threshold : Optional, threshold value (must be float between -1.0 and 1.0). Default is 0.9 scores : Optional, boolean indicating whether similarity scores are to be returned along with retrieved duplicates. outfile : Optional, name of the file to save the results, must be a json. Default is None. recursive : Optional, find images recursively in the image directory. Returns dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..} Example usage from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , min_similarity_threshold = 0.85 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to cnn encodings > , min_similarity_threshold = 0.85 , scores = True , outfile = 'results.json' ) find_duplicates_to_remove def find_duplicates_to_remove ( image_dir , encoding_map , min_similarity_threshold , outfile , recursive ) Give out a list of image file names to remove based on the similarity threshold. Does not remove the mentioned files. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as numpy arrays which represent the CNN encoding for the key image file. encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding CNN encodings. min_similarity_threshold : Optional, threshold value (must be float between -1.0 and 1.0). Default is 0.9 outfile : Optional, name of the file to save the results, must be a json. Default is None. recursive : Optional, find images recursively in the image directory. Returns duplicates : List of image file names that should be removed. Example usage from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), min_similarity_threshold = 0.85 ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( encoding_map =< mapping filename to cnn encodings > , min_similarity_threshold = 0.85 , outfile = 'results.json' )","title":"CNN"},{"location":"methods/cnn/#class-cnn","text":"Find duplicates using CNN and/or generate CNN encodings given a single image or a directory of images. The module can be used for 2 purposes: Encoding generation and duplicate detection. Encodings generation: To propagate an image through a Convolutional Neural Network architecture and generate encodings. The generated encodings can be used at a later time for deduplication. Using the method 'encode_image', the CNN encodings for a single image can be obtained while the 'encode_images' method can be used to get encodings for all images in a directory. Duplicate detection: Find duplicates either using the encoding mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplciates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks.","title":"class CNN"},{"location":"methods/cnn/#__init__","text":"def __init__ ( verbose ) Initialize a pytorch MobileNet model v3 that is sliced at the last convolutional layer. Set the batch size for pytorch dataloader to be 64 samples.","title":"__init__"},{"location":"methods/cnn/#args","text":"verbose : Display progress bar if True else disable it. Default value is True.","title":"Args"},{"location":"methods/cnn/#apply_mobilenet_preprocess","text":"def apply_mobilenet_preprocess ( im_arr )","title":"apply_mobilenet_preprocess"},{"location":"methods/cnn/#encode_image","text":"def encode_image ( image_file , image_array ) Generate CNN encoding for a single image.","title":"encode_image"},{"location":"methods/cnn/#args_1","text":"image_file : Path to the image file. image_array : Optional, used instead of image_file. Image typecast to numpy array.","title":"Args"},{"location":"methods/cnn/#returns","text":"encoding : Encodings for the image in the form of numpy array.","title":"Returns"},{"location":"methods/cnn/#example-usage","text":"from imagededup.methods import CNN myencoder = CNN () encoding = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR encoding = myencoder . encode_image ( image_array =< numpy array of image > )","title":"Example usage"},{"location":"methods/cnn/#encode_images","text":"def encode_images ( image_dir , recursive ) Generate CNN encodings for all images in a given directory of images. Test.","title":"encode_images"},{"location":"methods/cnn/#args_2","text":"image_dir : Path to the image directory. recursive : Optional, find images recursively in the image directory.","title":"Args"},{"location":"methods/cnn/#returns_1","text":"dictionary : Contains a mapping of filenames and corresponding numpy array of CNN encodings.","title":"Returns"},{"location":"methods/cnn/#example-usage_1","text":"from imagededup.methods import CNN myencoder = CNN () encoding_map = myencoder . encode_images ( image_dir = 'path/to/image/directory' )","title":"Example usage"},{"location":"methods/cnn/#find_duplicates","text":"def find_duplicates ( image_dir , encoding_map , min_similarity_threshold , scores , outfile , recursive ) Find duplicates for each file. Take in path of the directory or encoding dictionary in which duplicates are to be detected above the given threshold. Return dictionary containing key as filename and value as a list of duplicate file names. Optionally, the cosine distances could be returned instead of just duplicate filenames for each query file.","title":"find_duplicates"},{"location":"methods/cnn/#args_3","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding CNN encodings. min_similarity_threshold : Optional, threshold value (must be float between -1.0 and 1.0). Default is 0.9 scores : Optional, boolean indicating whether similarity scores are to be returned along with retrieved duplicates. outfile : Optional, name of the file to save the results, must be a json. Default is None. recursive : Optional, find images recursively in the image directory.","title":"Args"},{"location":"methods/cnn/#returns_2","text":"dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..}","title":"Returns"},{"location":"methods/cnn/#example-usage_2","text":"from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , min_similarity_threshold = 0.85 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to cnn encodings > , min_similarity_threshold = 0.85 , scores = True , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/cnn/#find_duplicates_to_remove","text":"def find_duplicates_to_remove ( image_dir , encoding_map , min_similarity_threshold , outfile , recursive ) Give out a list of image file names to remove based on the similarity threshold. Does not remove the mentioned files.","title":"find_duplicates_to_remove"},{"location":"methods/cnn/#args_4","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as numpy arrays which represent the CNN encoding for the key image file. encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding CNN encodings. min_similarity_threshold : Optional, threshold value (must be float between -1.0 and 1.0). Default is 0.9 outfile : Optional, name of the file to save the results, must be a json. Default is None. recursive : Optional, find images recursively in the image directory.","title":"Args"},{"location":"methods/cnn/#returns_3","text":"duplicates : List of image file names that should be removed.","title":"Returns"},{"location":"methods/cnn/#example-usage_3","text":"from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), min_similarity_threshold = 0.85 ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( encoding_map =< mapping filename to cnn encodings > , min_similarity_threshold = 0.85 , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/hashing/","text":"class Hashing Find duplicates using hashing algorithms and/or generate hashes given a single image or a directory of images. The module can be used for 2 purposes: Encoding generation and duplicate detection. Encoding generation: To generate hashes using specific hashing method. The generated hashes can be used at a later time for deduplication. Using the method 'encode_image' from the specific hashing method object, the hash for a single image can be obtained while the 'encode_images' method can be used to get hashes for all images in a directory. Duplicate detection: Find duplicates either using the encoding mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplicates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks. __init__ def __init__ ( verbose ) Initialize hashing class. Args verbose : Display progress bar if True else disable it. Default value is True. hamming_distance def hamming_distance ( hash1 , hash2 ) Calculate the hamming distance between two hashes. If length of hashes is not 64 bits, then pads the length to be 64 for each hash and then calculates the hamming distance. Args hash1 : hash string hash2 : hash string Returns hamming_distance : Hamming distance between the two hashes. encode_image def encode_image ( image_file , image_array ) Generate hash for a single image. Args image_file : Path to the image file. image_array : Optional, used instead of image_file. Image typecast to numpy array. Returns hash : A 16 character hexadecimal string hash for the image. Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () myhash = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR myhash = myencoder . encode_image ( image_array =< numpy array of image > ) encode_images def encode_images ( image_dir , recursive ) Generate hashes for all images in a given directory of images. Args image_dir : Path to the image directory. recursive : Optional, find images recursively in the image directory. Returns dictionary : A dictionary that contains a mapping of filenames and corresponding 64 character hash string such as {'Image1.jpg': 'hash_string1', 'Image2.jpg': 'hash_string2', ...} Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () mapping = myencoder . encode_images ( 'path/to/directory' ) find_duplicates def find_duplicates ( image_dir , encoding_map , max_distance_threshold , scores , outfile , search_method , recursive ) Find duplicates for each file. Takes in path of the directory or encoding dictionary in which duplicates are to be detected. All images with hamming distance less than or equal to the max_distance_threshold are regarded as duplicates. Returns dictionary containing key as filename and value as a list of duplicate file names. Optionally, the below the given hamming distance could be returned instead of just duplicate filenames for each query file. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Optional, hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. scores : Optional, boolean indicating whether Hamming distances are to be returned along with retrieved duplicates. outfile : Optional, name of the file to save the results, must be a json. Default is None. search_method : Algorithm used to retrieve duplicates. Default is brute_force_cython for Unix else bktree. recursive : Optional, find images recursively in the image directory. Returns duplicates dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..} Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , scores = True , outfile = 'results.json' ) find_duplicates_to_remove def find_duplicates_to_remove ( image_dir , encoding_map , max_distance_threshold , outfile , recursive ) Give out a list of image file names to remove based on the hamming distance threshold threshold. Does not remove the mentioned files. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Optional, hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. outfile : Optional, name of the file to save the results, must be a json. Default is None. recursive : Optional, find images recursively in the image directory. Returns duplicates : List of image file names that are found to be duplicate of me other file in the directory. Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), max_distance_threshold = 15 ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , outfile = 'results.json' ) class PHash Inherits from Hashing base class and implements perceptual hashing (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html). Offers all the functionality mentioned in hashing class. Example usage # Perceptual hash for images from imagededup.methods import PHash phasher = PHash () perceptual_hash = phasher . encode_image ( image_file = 'path/to/image.jpg' ) OR perceptual_hash = phasher . encode_image ( image_array = < numpy image array > ) OR perceptual_hashes = phasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = phasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import PHash phasher = PHash () files_to_remove = phasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = phasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ ( verbose ) Initialize perceptual hashing class. Args verbose : Display progress bar if True else disable it. Default value is True. class AHash Inherits from Hashing base class and implements average hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class. Example usage # Average hash for images from imagededup.methods import AHash ahasher = AHash () average_hash = ahasher . encode_image ( image_file = 'path/to/image.jpg' ) OR average_hash = ahasher . encode_image ( image_array = < numpy image array > ) OR average_hashes = ahasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import AHash ahasher = AHash () duplicates = ahasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = ahasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import AHash ahasher = AHash () files_to_remove = ahasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = ahasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ ( verbose ) Initialize average hashing class. Args verbose : Display progress bar if True else disable it. Default value is True. class DHash Inherits from Hashing base class and implements difference hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class. Example usage # Difference hash for images from imagededup.methods import DHash dhasher = DHash () difference_hash = dhasher . encode_image ( image_file = 'path/to/image.jpg' ) OR difference_hash = dhasher . encode_image ( image_array = < numpy image array > ) OR difference_hashes = dhasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import DHash dhasher = DHash () duplicates = dhasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = dhasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import DHash dhasher = DHash () files_to_remove = dhasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = dhasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ ( verbose ) Initialize difference hashing class. Args verbose : Display progress bar if True else disable it. Default value is True. class WHash Inherits from Hashing base class and implements wavelet hashing. (Implementation reference: https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5) Offers all the functionality mentioned in hashing class. Example usage # Wavelet hash for images from imagededup.methods import WHash whasher = WHash () wavelet_hash = whasher . encode_image ( image_file = 'path/to/image.jpg' ) OR wavelet_hash = whasher . encode_image ( image_array = < numpy image array > ) OR wavelet_hashes = whasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import WHash whasher = WHash () duplicates = whasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = whasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import WHash whasher = WHash () files_to_remove = whasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = whasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ ( verbose ) Initialize wavelet hashing class. Args verbose : Display progress bar if True else disable it. Default value is True.","title":"Hashing"},{"location":"methods/hashing/#class-hashing","text":"Find duplicates using hashing algorithms and/or generate hashes given a single image or a directory of images. The module can be used for 2 purposes: Encoding generation and duplicate detection. Encoding generation: To generate hashes using specific hashing method. The generated hashes can be used at a later time for deduplication. Using the method 'encode_image' from the specific hashing method object, the hash for a single image can be obtained while the 'encode_images' method can be used to get hashes for all images in a directory. Duplicate detection: Find duplicates either using the encoding mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplicates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks.","title":"class Hashing"},{"location":"methods/hashing/#__init__","text":"def __init__ ( verbose ) Initialize hashing class.","title":"__init__"},{"location":"methods/hashing/#args","text":"verbose : Display progress bar if True else disable it. Default value is True.","title":"Args"},{"location":"methods/hashing/#hamming_distance","text":"def hamming_distance ( hash1 , hash2 ) Calculate the hamming distance between two hashes. If length of hashes is not 64 bits, then pads the length to be 64 for each hash and then calculates the hamming distance.","title":"hamming_distance"},{"location":"methods/hashing/#args_1","text":"hash1 : hash string hash2 : hash string","title":"Args"},{"location":"methods/hashing/#returns","text":"hamming_distance : Hamming distance between the two hashes.","title":"Returns"},{"location":"methods/hashing/#encode_image","text":"def encode_image ( image_file , image_array ) Generate hash for a single image.","title":"encode_image"},{"location":"methods/hashing/#args_2","text":"image_file : Path to the image file. image_array : Optional, used instead of image_file. Image typecast to numpy array.","title":"Args"},{"location":"methods/hashing/#returns_1","text":"hash : A 16 character hexadecimal string hash for the image.","title":"Returns"},{"location":"methods/hashing/#example-usage","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () myhash = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR myhash = myencoder . encode_image ( image_array =< numpy array of image > )","title":"Example usage"},{"location":"methods/hashing/#encode_images","text":"def encode_images ( image_dir , recursive ) Generate hashes for all images in a given directory of images.","title":"encode_images"},{"location":"methods/hashing/#args_3","text":"image_dir : Path to the image directory. recursive : Optional, find images recursively in the image directory.","title":"Args"},{"location":"methods/hashing/#returns_2","text":"dictionary : A dictionary that contains a mapping of filenames and corresponding 64 character hash string such as {'Image1.jpg': 'hash_string1', 'Image2.jpg': 'hash_string2', ...}","title":"Returns"},{"location":"methods/hashing/#example-usage_1","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () mapping = myencoder . encode_images ( 'path/to/directory' )","title":"Example usage"},{"location":"methods/hashing/#find_duplicates","text":"def find_duplicates ( image_dir , encoding_map , max_distance_threshold , scores , outfile , search_method , recursive ) Find duplicates for each file. Takes in path of the directory or encoding dictionary in which duplicates are to be detected. All images with hamming distance less than or equal to the max_distance_threshold are regarded as duplicates. Returns dictionary containing key as filename and value as a list of duplicate file names. Optionally, the below the given hamming distance could be returned instead of just duplicate filenames for each query file.","title":"find_duplicates"},{"location":"methods/hashing/#args_4","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Optional, hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. scores : Optional, boolean indicating whether Hamming distances are to be returned along with retrieved duplicates. outfile : Optional, name of the file to save the results, must be a json. Default is None. search_method : Algorithm used to retrieve duplicates. Default is brute_force_cython for Unix else bktree. recursive : Optional, find images recursively in the image directory.","title":"Args"},{"location":"methods/hashing/#returns_3","text":"duplicates dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..}","title":"Returns"},{"location":"methods/hashing/#example-usage_2","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , scores = True , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/hashing/#find_duplicates_to_remove","text":"def find_duplicates_to_remove ( image_dir , encoding_map , max_distance_threshold , outfile , recursive ) Give out a list of image file names to remove based on the hamming distance threshold threshold. Does not remove the mentioned files.","title":"find_duplicates_to_remove"},{"location":"methods/hashing/#args_5","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : Optional, used instead of image_dir, a dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Optional, hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. outfile : Optional, name of the file to save the results, must be a json. Default is None. recursive : Optional, find images recursively in the image directory.","title":"Args"},{"location":"methods/hashing/#returns_4","text":"duplicates : List of image file names that are found to be duplicate of me other file in the directory.","title":"Returns"},{"location":"methods/hashing/#example-usage_3","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), max_distance_threshold = 15 ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/hashing/#class-phash","text":"Inherits from Hashing base class and implements perceptual hashing (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html). Offers all the functionality mentioned in hashing class.","title":"class PHash"},{"location":"methods/hashing/#example-usage_4","text":"# Perceptual hash for images from imagededup.methods import PHash phasher = PHash () perceptual_hash = phasher . encode_image ( image_file = 'path/to/image.jpg' ) OR perceptual_hash = phasher . encode_image ( image_array = < numpy image array > ) OR perceptual_hashes = phasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = phasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import PHash phasher = PHash () files_to_remove = phasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = phasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#__init___1","text":"def __init__ ( verbose ) Initialize perceptual hashing class.","title":"__init__"},{"location":"methods/hashing/#args_6","text":"verbose : Display progress bar if True else disable it. Default value is True.","title":"Args"},{"location":"methods/hashing/#class-ahash","text":"Inherits from Hashing base class and implements average hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class.","title":"class AHash"},{"location":"methods/hashing/#example-usage_5","text":"# Average hash for images from imagededup.methods import AHash ahasher = AHash () average_hash = ahasher . encode_image ( image_file = 'path/to/image.jpg' ) OR average_hash = ahasher . encode_image ( image_array = < numpy image array > ) OR average_hashes = ahasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import AHash ahasher = AHash () duplicates = ahasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = ahasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import AHash ahasher = AHash () files_to_remove = ahasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = ahasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#__init___2","text":"def __init__ ( verbose ) Initialize average hashing class.","title":"__init__"},{"location":"methods/hashing/#args_7","text":"verbose : Display progress bar if True else disable it. Default value is True.","title":"Args"},{"location":"methods/hashing/#class-dhash","text":"Inherits from Hashing base class and implements difference hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class.","title":"class DHash"},{"location":"methods/hashing/#example-usage_6","text":"# Difference hash for images from imagededup.methods import DHash dhasher = DHash () difference_hash = dhasher . encode_image ( image_file = 'path/to/image.jpg' ) OR difference_hash = dhasher . encode_image ( image_array = < numpy image array > ) OR difference_hashes = dhasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import DHash dhasher = DHash () duplicates = dhasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = dhasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import DHash dhasher = DHash () files_to_remove = dhasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = dhasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#__init___3","text":"def __init__ ( verbose ) Initialize difference hashing class.","title":"__init__"},{"location":"methods/hashing/#args_8","text":"verbose : Display progress bar if True else disable it. Default value is True.","title":"Args"},{"location":"methods/hashing/#class-whash","text":"Inherits from Hashing base class and implements wavelet hashing. (Implementation reference: https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5) Offers all the functionality mentioned in hashing class.","title":"class WHash"},{"location":"methods/hashing/#example-usage_7","text":"# Wavelet hash for images from imagededup.methods import WHash whasher = WHash () wavelet_hash = whasher . encode_image ( image_file = 'path/to/image.jpg' ) OR wavelet_hash = whasher . encode_image ( image_array = < numpy image array > ) OR wavelet_hashes = whasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import WHash whasher = WHash () duplicates = whasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = whasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import WHash whasher = WHash () files_to_remove = whasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = whasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#__init___4","text":"def __init__ ( verbose ) Initialize wavelet hashing class.","title":"__init__"},{"location":"methods/hashing/#args_9","text":"verbose : Display progress bar if True else disable it. Default value is True.","title":"Args"},{"location":"user_guide/benchmarks/","text":"Benchmarks To gauge an idea of the speed and accuracy of the implemented algorithms, a benchmark has been provided on the UKBench dataset (zip file titled 'UKBench image collection' having size ~1.5G) and some variations derived from it. Datasets 3 datasets that have been used: Near duplicate dataset ( UKBench dataset ): This dataset has near duplicates that are arranged in groups of 4. There are a total of 2550 such groups amounting to a total of 10200 RGB images. The size of each image is 640 x 480 with jpg extension. The image below depicts 3 example groups from the UKBench dataset. Each row represents a group with the corresponding 4 images from the group. Transformed dataset derived from UKBench dataset: An image from different groups of the UKBench dataset was taken and the following 5 transformations were applied to the original image: Random crop preserving the original aspect ratio (new size - 560 x 420) Horizontal flip Vertical flip 25 degree rotation Resizing with change in aspect ratio (new aspect ratio - 1:1) Thus, each group has a total of 6 images (original + transformed). A total of 1800 such groups were created totalling 10800 images in the dataset. Exact duplicate dataset: An image from each of the 2550 image groups of the UKBench dataset was taken and an exact duplicate was created. The number of images totalled 5100. Environment The benchmarks were performed on an AWS ec2 r5.xlarge instance having 4 vCPUs and 32 GB memory. The instance does not have a GPU, so all the runs are done on CPUs. Metrics The metrics used here are classification metrics as explained in the documentation . class-0 refers to non-duplicate image pairs. class-1 refers to duplicate image pairs. The reported numbers are rounded off to nearest 3 digits. Timings The times are reported in seconds and comprise the time taken to generate encodings and find duplicates. The time taken to perform the evaluation task is NOT reported. Threshold selection For each method, 3 different thresholds have been selected. For hashing methods, following max_distance_threshold values are used: 0: Indicates that exactly the same hash should be generated for the image pairs to be considered duplicates. 10: Default. 32: Halfway between the maximum and minimum values (0 and 64). For cnn method, following min_similarity_threshold values are used: 1.0: Indicates that exactly the same cnn embeddings should be generated for the image pairs to be considered duplicates. 0.9: Default. 0.5: A threshold that allows large deviation between image pairs. Results Near Duplicate dataset Method Threshold Time (s) class-0 precision class-1 precision class-0 recall class-1 recall dhash 0 35.570 0.999 0.0 1.0 0.0 dhash 10 35.810 0.999 0.018 0.999 0.0461 dhash 32 106.670 0.998 0.0 0.326 0.884 phash 0 40.073 0.999 1.0 1.0 0.0 phash 10 39.056 0.999 0.498 0.999 0.016 phash 32 98.835 0.998 0.0 0.343 0.856 ahash 0 36.171 0.999 0.282 0.999 0.002 ahash 10 36.560 0.999 0.012 0.996 0.193 ahash 32 97.170 0.999 0.000 0.448 0.932 whash 0 51.710 0.999 0.112 0.999 0.002 whash 10 51.940 0.999 0.008 0.993 0.199 whash 32 112.560 0.999 0.0 0.416 0.933 cnn 0.5 379.680 0.999 0.0 0.856 0.999 cnn 0.9 377.157 0.999 0.995 0.999 0.127 cnn 1.0 379.570 0.999 0.0 1.0 0.0 Observations The cnn method with a threshold between 0.5 and 0.9 would work best for finding near duplicates. This is indicated by the extreme values class-1 precision and recall takes for the two thresholds. Hashing methods do not perform well for finding near duplicates. Transformed dataset Method Threshold Time (s) class-0 precision class-1 precision class-0 recall class-1 recall dhash 0 25.360 0.999 1.0 1.0 0.040 dhash 10 25.309 0.999 0.138 0.999 0.117 dhash 32 108.960 0.990 0.0 0.336 0.872 phash 0 28.069 0.999 1.0 1.0 0.050 phash 10 28.075 0.999 0.341 0.999 0.079 phash 32 107.079 0.990 0.003 0.328 0.847 ahash 0 25.270 0.999 0.961 0.999 0.058 ahash 10 25.389 0.999 0.035 0.997 0.216 ahash 32 93.084 0.990 0.0 0.441 0.849 whash 0 40.390 0.999 0.917 0.999 0.061 whash 10 41.260 0.999 0.023 0.996 0.203 whash 32 109.630 0.990 0.0 0.410 0.853 cnn 0.5 397.380 0.999 0.003 0.852 0.999 cnn 0.9 392.090 0.999 0.999 0.990 0.384 cnn 1.0 396.250 0.990 0.0 1.0 0.0 Observations The cnn method with threshold 0.9 seems to work best for finding transformed duplicates. A slightly lower min_similarity_threshold value could lead to a higher class-1 recall. Hashing methods do not perform well for finding transformed duplicates. In reality, resized images get found easily, but all other transformations lead to a bad performance for hashing methods. Exact duplicates dataset Method Threshold Time (s) class-0 precision class-1 precision class-0 recall class-1 recall dhash 0 18.380 1.0 1.0 1.0 1.0 dhash 10 18.410 1.0 0.223 0.999 1.0 dhash 32 34.602 1.0 0.0 0.327 1.0 phash 0 19.780 1.0 1.0 1.0 1.0 phash 10 20.012 1.0 0.980 0.999 1.0 phash 32 34.054 1.0 0.0 0.344 1.0 ahash 0 18.180 1.0 0.998 0.999 1.0 ahash 10 18.228 1.0 0.044 0.995 1.0 ahash 32 31.961 1.0 0.0 0.448 1.0 whash 0 26.097 1.0 0.980 0.999 1.0 whash 10 26.056 1.0 0.029 0.993 1.0 whash 32 39.408 1.0 0.0 0.417 1.0 cnn 0.5 192.050 1.0 0.001 0.860 1.0 cnn 0.9 191.024 1.0 1.0 1.0 1.0 cnn 1.0 194.270 0.999 1.0 1.0 0.580* * The value is low as opposed to the expected 1.0 because of the cosine_similarity function from scikit-learn (used within the package) which sometimes calculates the similarity to be slightly less than 1.0 even when the vectors are same. Observations Difference hashing is the fastest ( max_distance_threshold 0). When using hashing methods for exact duplicates, keep max_distance_threshold to a low value. The value of 0 is good, but a slightly higher value should also work fine. When using cnn method, keep min_similarity_threshold to a high value. The default value of 0.9 seems to work well. A slightly higher value can also be used. Summary Near duplicate dataset: use cnn with an appropriate min_similarity_threshold . Transformed dataset: use cnn with min_similarity_threshold of around 0.9 (default). Exact duplicates dataset: use Difference hashing with 0 max_distance_threshold . A higher max_distance_threshold (i.e., hashing) leads to a higher execution time. cnn method doesn't seem much affected by the min_similarity_threshold (though a lower value would add a few seconds to the execution time as can be seen in all the runs above.) Generally speaking, the cnn method takes longer to run as compared to hashing methods for all datasets. If a GPU is available, cnn method should be much faster.","title":"Benchmarks"},{"location":"user_guide/benchmarks/#benchmarks","text":"To gauge an idea of the speed and accuracy of the implemented algorithms, a benchmark has been provided on the UKBench dataset (zip file titled 'UKBench image collection' having size ~1.5G) and some variations derived from it.","title":"Benchmarks"},{"location":"user_guide/benchmarks/#datasets","text":"3 datasets that have been used: Near duplicate dataset ( UKBench dataset ): This dataset has near duplicates that are arranged in groups of 4. There are a total of 2550 such groups amounting to a total of 10200 RGB images. The size of each image is 640 x 480 with jpg extension. The image below depicts 3 example groups from the UKBench dataset. Each row represents a group with the corresponding 4 images from the group. Transformed dataset derived from UKBench dataset: An image from different groups of the UKBench dataset was taken and the following 5 transformations were applied to the original image: Random crop preserving the original aspect ratio (new size - 560 x 420) Horizontal flip Vertical flip 25 degree rotation Resizing with change in aspect ratio (new aspect ratio - 1:1) Thus, each group has a total of 6 images (original + transformed). A total of 1800 such groups were created totalling 10800 images in the dataset. Exact duplicate dataset: An image from each of the 2550 image groups of the UKBench dataset was taken and an exact duplicate was created. The number of images totalled 5100.","title":"Datasets"},{"location":"user_guide/benchmarks/#environment","text":"The benchmarks were performed on an AWS ec2 r5.xlarge instance having 4 vCPUs and 32 GB memory. The instance does not have a GPU, so all the runs are done on CPUs.","title":"Environment"},{"location":"user_guide/benchmarks/#metrics","text":"The metrics used here are classification metrics as explained in the documentation . class-0 refers to non-duplicate image pairs. class-1 refers to duplicate image pairs. The reported numbers are rounded off to nearest 3 digits.","title":"Metrics"},{"location":"user_guide/benchmarks/#timings","text":"The times are reported in seconds and comprise the time taken to generate encodings and find duplicates. The time taken to perform the evaluation task is NOT reported.","title":"Timings"},{"location":"user_guide/benchmarks/#threshold-selection","text":"For each method, 3 different thresholds have been selected. For hashing methods, following max_distance_threshold values are used: 0: Indicates that exactly the same hash should be generated for the image pairs to be considered duplicates. 10: Default. 32: Halfway between the maximum and minimum values (0 and 64). For cnn method, following min_similarity_threshold values are used: 1.0: Indicates that exactly the same cnn embeddings should be generated for the image pairs to be considered duplicates. 0.9: Default. 0.5: A threshold that allows large deviation between image pairs.","title":"Threshold selection"},{"location":"user_guide/benchmarks/#results","text":"","title":"Results"},{"location":"user_guide/benchmarks/#near-duplicate-dataset","text":"Method Threshold Time (s) class-0 precision class-1 precision class-0 recall class-1 recall dhash 0 35.570 0.999 0.0 1.0 0.0 dhash 10 35.810 0.999 0.018 0.999 0.0461 dhash 32 106.670 0.998 0.0 0.326 0.884 phash 0 40.073 0.999 1.0 1.0 0.0 phash 10 39.056 0.999 0.498 0.999 0.016 phash 32 98.835 0.998 0.0 0.343 0.856 ahash 0 36.171 0.999 0.282 0.999 0.002 ahash 10 36.560 0.999 0.012 0.996 0.193 ahash 32 97.170 0.999 0.000 0.448 0.932 whash 0 51.710 0.999 0.112 0.999 0.002 whash 10 51.940 0.999 0.008 0.993 0.199 whash 32 112.560 0.999 0.0 0.416 0.933 cnn 0.5 379.680 0.999 0.0 0.856 0.999 cnn 0.9 377.157 0.999 0.995 0.999 0.127 cnn 1.0 379.570 0.999 0.0 1.0 0.0","title":"Near Duplicate dataset"},{"location":"user_guide/benchmarks/#observations","text":"The cnn method with a threshold between 0.5 and 0.9 would work best for finding near duplicates. This is indicated by the extreme values class-1 precision and recall takes for the two thresholds. Hashing methods do not perform well for finding near duplicates.","title":"Observations"},{"location":"user_guide/benchmarks/#transformed-dataset","text":"Method Threshold Time (s) class-0 precision class-1 precision class-0 recall class-1 recall dhash 0 25.360 0.999 1.0 1.0 0.040 dhash 10 25.309 0.999 0.138 0.999 0.117 dhash 32 108.960 0.990 0.0 0.336 0.872 phash 0 28.069 0.999 1.0 1.0 0.050 phash 10 28.075 0.999 0.341 0.999 0.079 phash 32 107.079 0.990 0.003 0.328 0.847 ahash 0 25.270 0.999 0.961 0.999 0.058 ahash 10 25.389 0.999 0.035 0.997 0.216 ahash 32 93.084 0.990 0.0 0.441 0.849 whash 0 40.390 0.999 0.917 0.999 0.061 whash 10 41.260 0.999 0.023 0.996 0.203 whash 32 109.630 0.990 0.0 0.410 0.853 cnn 0.5 397.380 0.999 0.003 0.852 0.999 cnn 0.9 392.090 0.999 0.999 0.990 0.384 cnn 1.0 396.250 0.990 0.0 1.0 0.0","title":"Transformed dataset"},{"location":"user_guide/benchmarks/#observations_1","text":"The cnn method with threshold 0.9 seems to work best for finding transformed duplicates. A slightly lower min_similarity_threshold value could lead to a higher class-1 recall. Hashing methods do not perform well for finding transformed duplicates. In reality, resized images get found easily, but all other transformations lead to a bad performance for hashing methods.","title":"Observations"},{"location":"user_guide/benchmarks/#exact-duplicates-dataset","text":"Method Threshold Time (s) class-0 precision class-1 precision class-0 recall class-1 recall dhash 0 18.380 1.0 1.0 1.0 1.0 dhash 10 18.410 1.0 0.223 0.999 1.0 dhash 32 34.602 1.0 0.0 0.327 1.0 phash 0 19.780 1.0 1.0 1.0 1.0 phash 10 20.012 1.0 0.980 0.999 1.0 phash 32 34.054 1.0 0.0 0.344 1.0 ahash 0 18.180 1.0 0.998 0.999 1.0 ahash 10 18.228 1.0 0.044 0.995 1.0 ahash 32 31.961 1.0 0.0 0.448 1.0 whash 0 26.097 1.0 0.980 0.999 1.0 whash 10 26.056 1.0 0.029 0.993 1.0 whash 32 39.408 1.0 0.0 0.417 1.0 cnn 0.5 192.050 1.0 0.001 0.860 1.0 cnn 0.9 191.024 1.0 1.0 1.0 1.0 cnn 1.0 194.270 0.999 1.0 1.0 0.580* * The value is low as opposed to the expected 1.0 because of the cosine_similarity function from scikit-learn (used within the package) which sometimes calculates the similarity to be slightly less than 1.0 even when the vectors are same.","title":"Exact duplicates dataset"},{"location":"user_guide/benchmarks/#observations_2","text":"Difference hashing is the fastest ( max_distance_threshold 0). When using hashing methods for exact duplicates, keep max_distance_threshold to a low value. The value of 0 is good, but a slightly higher value should also work fine. When using cnn method, keep min_similarity_threshold to a high value. The default value of 0.9 seems to work well. A slightly higher value can also be used.","title":"Observations"},{"location":"user_guide/benchmarks/#summary","text":"Near duplicate dataset: use cnn with an appropriate min_similarity_threshold . Transformed dataset: use cnn with min_similarity_threshold of around 0.9 (default). Exact duplicates dataset: use Difference hashing with 0 max_distance_threshold . A higher max_distance_threshold (i.e., hashing) leads to a higher execution time. cnn method doesn't seem much affected by the min_similarity_threshold (though a lower value would add a few seconds to the execution time as can be seen in all the runs above.) Generally speaking, the cnn method takes longer to run as compared to hashing methods for all datasets. If a GPU is available, cnn method should be much faster.","title":"Summary"},{"location":"user_guide/encoding_generation/","text":"Encoding generation It might be desirable to only generate the hashes/cnn encodings for a given image or all images in a directory instead of directly deduplicating using find_duplicates method. Encodings can be generated for a directory of images or for a single image: Encoding generation for all images in a directory Encoding generation for a single image Encoding generation for all images in a directory To generate encodings for all images in an image directory encode_images function can be used. The general api for using encode_images is: from imagededup.methods import < method - name > method_object = < method - name > () encodings = method_object . encode_images ( image_dir = 'path/to/image/directory' ) where the returned variable encodings is a dictionary mapping image file names to corresponding encoding: { 'image1.jpg': <encoding-image-1>, 'image2.jpg': <encoding-image-2>, .. } For hashing algorithms, the encodings are 64 bit hashes represented as 16 character hexadecimal strings. For cnn, the encodings are numpy array with shape (576,). The 'method-name' corresponds to one of the deduplication methods available and can be set to: PHash AHash DHash WHash CNN Options image_dir: Path to the image directory for which encodings are to be generated. Considerations If an image in the image directory can't be loaded, no encodings are generated for the image. Hence, there is no entry for the image in the returned encodings dictionary. Supported image formats: 'JPEG', 'PNG', 'BMP', 'MPO', 'PPM', 'TIFF', 'GIF', 'SVG', 'PGM', 'PBM'. Examples Generating encodings using Difference hash: from imagededup.methods import DHash dhasher = DHash () encodings = dhasher . encode_images ( image_dir = 'path/to/image/directory' ) Encoding generation for a single image To generate encodings for a single image encode_image function can be used. The general api for using encode_image is: from imagededup.methods import < method - name > method_object = < method - name > () encoding = method_object . encode_image ( image_file = 'path/to/image/file' ) where the returned variable encoding is either a hexadecimal string if a hashing method is used or a (576,) numpy array if cnn is used. Options image_file: Optional, path to the image file for which encodings are to be generated. image_array: Optional, used instead of image_file attribute. A numpy array representing the image. Considerations If the image can't be loaded, no encodings are generated for the image and None is returned. Supported image formats: 'JPEG', 'PNG', 'BMP', 'MPO', 'PPM', 'TIFF', 'GIF', 'SVG', 'PGM', 'PBM'. Examples Generating encodings using Difference hash: from imagededup.methods import DHash dhasher = DHash () encoding = dhasher . encode_image ( image_file = 'path/to/image/file' )","title":"Encoding generation"},{"location":"user_guide/encoding_generation/#encoding-generation","text":"It might be desirable to only generate the hashes/cnn encodings for a given image or all images in a directory instead of directly deduplicating using find_duplicates method. Encodings can be generated for a directory of images or for a single image: Encoding generation for all images in a directory Encoding generation for a single image","title":"Encoding generation"},{"location":"user_guide/encoding_generation/#encoding-generation-for-all-images-in-a-directory","text":"To generate encodings for all images in an image directory encode_images function can be used. The general api for using encode_images is: from imagededup.methods import < method - name > method_object = < method - name > () encodings = method_object . encode_images ( image_dir = 'path/to/image/directory' ) where the returned variable encodings is a dictionary mapping image file names to corresponding encoding: { 'image1.jpg': <encoding-image-1>, 'image2.jpg': <encoding-image-2>, .. } For hashing algorithms, the encodings are 64 bit hashes represented as 16 character hexadecimal strings. For cnn, the encodings are numpy array with shape (576,). The 'method-name' corresponds to one of the deduplication methods available and can be set to: PHash AHash DHash WHash CNN","title":"Encoding generation for all images in a directory"},{"location":"user_guide/encoding_generation/#options","text":"image_dir: Path to the image directory for which encodings are to be generated.","title":"Options"},{"location":"user_guide/encoding_generation/#considerations","text":"If an image in the image directory can't be loaded, no encodings are generated for the image. Hence, there is no entry for the image in the returned encodings dictionary. Supported image formats: 'JPEG', 'PNG', 'BMP', 'MPO', 'PPM', 'TIFF', 'GIF', 'SVG', 'PGM', 'PBM'.","title":"Considerations"},{"location":"user_guide/encoding_generation/#examples","text":"Generating encodings using Difference hash: from imagededup.methods import DHash dhasher = DHash () encodings = dhasher . encode_images ( image_dir = 'path/to/image/directory' )","title":"Examples"},{"location":"user_guide/encoding_generation/#encoding-generation-for-a-single-image","text":"To generate encodings for a single image encode_image function can be used. The general api for using encode_image is: from imagededup.methods import < method - name > method_object = < method - name > () encoding = method_object . encode_image ( image_file = 'path/to/image/file' ) where the returned variable encoding is either a hexadecimal string if a hashing method is used or a (576,) numpy array if cnn is used.","title":"Encoding generation for a single image"},{"location":"user_guide/encoding_generation/#options_1","text":"image_file: Optional, path to the image file for which encodings are to be generated. image_array: Optional, used instead of image_file attribute. A numpy array representing the image.","title":"Options"},{"location":"user_guide/encoding_generation/#considerations_1","text":"If the image can't be loaded, no encodings are generated for the image and None is returned. Supported image formats: 'JPEG', 'PNG', 'BMP', 'MPO', 'PPM', 'TIFF', 'GIF', 'SVG', 'PGM', 'PBM'.","title":"Considerations"},{"location":"user_guide/encoding_generation/#examples_1","text":"Generating encodings using Difference hash: from imagededup.methods import DHash dhasher = DHash () encoding = dhasher . encode_image ( image_file = 'path/to/image/file' )","title":"Examples"},{"location":"user_guide/evaluating_performance/","text":"Evaluation of deduplication quality To determine the quality of deduplication algorithm and the corresponding threshold, an evaluation framework is provided. Given a ground truth mapping consisting of file names and a list of duplicates for each file along with a retrieved mapping from the deduplication algorithm for the same files, the following metrics can be obtained using the framework: Mean Average Precision (MAP) Mean Normalized Discounted Cumulative Gain (NDCG) Jaccard Index Per class Precision (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class Recall (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class f1-score (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) The api for obtaining these metrics is as below: from imagededup.evaluation import evaluate metrics = evaluate ( ground_truth_map , retrieved_map , metric = '<metric-name>' ) where the returned variable metrics is a dictionary containing the following content: { 'map': <map>, 'ndcg': <mean ndcg>, 'jaccard': <mean jaccard index>, 'precision': <numpy array having per class precision>, 'recall': <numpy array having per class recall>, 'f1-score': <numpy array having per class f1-score>, 'support': <numpy array having per class support> } Options ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric: Can take one of the following values: 'map' 'ndcg' 'jaccard' 'classification': Returns per class precision, recall, f1-score, support 'all' (default, returns all the above metrics) Considerations Presently, the ground truth map should be prepared manually by the user. Symmetric relations between duplicates must be represented in the ground truth map. If an image i is a duplicate of image j , then j must also be represented as a duplicate of i . Absence of symmetric relations will lead to an exception. Both the ground_truth_map and retrieved_map must have the same keys. There is a difference between the way information retrieval metrics(map, ndcg, jaccard index) and classification metrics(precision, recall, f1-score) treat the symmetric relationships in duplicates. Consider the following ground_truth_map and retrieved_map: ground_truth_map: { '1.jpg': ['2.jpg', '4.jpg'], '2.jpg': ['1.jpg'], '3.jpg': [], '4.jpg': ['1.jpg'] } retrieved_map: { '1.jpg': ['2.jpg'], '2.jpg': ['1.jpg'], '3.jpg': [], '4.jpg': [] } From the above, it can be seen that images '1.jpg' and '4.jpg' are not found to be duplicates of each other by the deduplication algorithm. For calculating information retrieval metrics, each key in the maps is considered as an independent 'query'. In the ground truth, '4.jpg' is a duplicate of the key '1.jpg' . When it is not retrieved, it is considered a miss for query '1.jpg' . Similarly, '1.jpg' is a duplicate of the key '4.jpg' in the ground truth. When this is not retrieved, it is considered a miss for query '4.jpg' . Thus, the missing relationship is accounted for twice instead of just once. Classification metrics, on the other hand, consider the relationships only once by forming unique pairs of images and labelling each pair as a 0 (non-duplicate image pair) and 1 (duplicate image pair). Using the ground_truth_map, the ground truth pairs with the corresponding labels are: Image Pair Label ('1.jpg', '2.jpg') 1 ('1.jpg', '3.jpg') 0 ('1.jpg', '4.jpg') 1 ('2.jpg', '3.jpg') 0 ('2.jpg', '4.jpg') 0 ('3.jpg', '4.jpg') 0 Similarly, using retrieved_map, the retrieved pairs are generated: Image Pair Label ('1.jpg', '2.jpg') 1 ('1.jpg', '3.jpg') 0 ('1.jpg', '4.jpg') 0 ('2.jpg', '3.jpg') 0 ('2.jpg', '4.jpg') 0 ('3.jpg', '4.jpg') 0 These two sets of pairs are then used to calculate metrics such as precision/recall/f1-score. It can be seen that the missing relationship between pair ('1jpg', '4.jpg') is accounted for only once.","title":"Evaluating performance"},{"location":"user_guide/evaluating_performance/#evaluation-of-deduplication-quality","text":"To determine the quality of deduplication algorithm and the corresponding threshold, an evaluation framework is provided. Given a ground truth mapping consisting of file names and a list of duplicates for each file along with a retrieved mapping from the deduplication algorithm for the same files, the following metrics can be obtained using the framework: Mean Average Precision (MAP) Mean Normalized Discounted Cumulative Gain (NDCG) Jaccard Index Per class Precision (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class Recall (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class f1-score (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) The api for obtaining these metrics is as below: from imagededup.evaluation import evaluate metrics = evaluate ( ground_truth_map , retrieved_map , metric = '<metric-name>' ) where the returned variable metrics is a dictionary containing the following content: { 'map': <map>, 'ndcg': <mean ndcg>, 'jaccard': <mean jaccard index>, 'precision': <numpy array having per class precision>, 'recall': <numpy array having per class recall>, 'f1-score': <numpy array having per class f1-score>, 'support': <numpy array having per class support> }","title":"Evaluation of deduplication quality"},{"location":"user_guide/evaluating_performance/#options","text":"ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric: Can take one of the following values: 'map' 'ndcg' 'jaccard' 'classification': Returns per class precision, recall, f1-score, support 'all' (default, returns all the above metrics)","title":"Options"},{"location":"user_guide/evaluating_performance/#considerations","text":"Presently, the ground truth map should be prepared manually by the user. Symmetric relations between duplicates must be represented in the ground truth map. If an image i is a duplicate of image j , then j must also be represented as a duplicate of i . Absence of symmetric relations will lead to an exception. Both the ground_truth_map and retrieved_map must have the same keys. There is a difference between the way information retrieval metrics(map, ndcg, jaccard index) and classification metrics(precision, recall, f1-score) treat the symmetric relationships in duplicates. Consider the following ground_truth_map and retrieved_map: ground_truth_map: { '1.jpg': ['2.jpg', '4.jpg'], '2.jpg': ['1.jpg'], '3.jpg': [], '4.jpg': ['1.jpg'] } retrieved_map: { '1.jpg': ['2.jpg'], '2.jpg': ['1.jpg'], '3.jpg': [], '4.jpg': [] } From the above, it can be seen that images '1.jpg' and '4.jpg' are not found to be duplicates of each other by the deduplication algorithm. For calculating information retrieval metrics, each key in the maps is considered as an independent 'query'. In the ground truth, '4.jpg' is a duplicate of the key '1.jpg' . When it is not retrieved, it is considered a miss for query '1.jpg' . Similarly, '1.jpg' is a duplicate of the key '4.jpg' in the ground truth. When this is not retrieved, it is considered a miss for query '4.jpg' . Thus, the missing relationship is accounted for twice instead of just once. Classification metrics, on the other hand, consider the relationships only once by forming unique pairs of images and labelling each pair as a 0 (non-duplicate image pair) and 1 (duplicate image pair). Using the ground_truth_map, the ground truth pairs with the corresponding labels are: Image Pair Label ('1.jpg', '2.jpg') 1 ('1.jpg', '3.jpg') 0 ('1.jpg', '4.jpg') 1 ('2.jpg', '3.jpg') 0 ('2.jpg', '4.jpg') 0 ('3.jpg', '4.jpg') 0 Similarly, using retrieved_map, the retrieved pairs are generated: Image Pair Label ('1.jpg', '2.jpg') 1 ('1.jpg', '3.jpg') 0 ('1.jpg', '4.jpg') 0 ('2.jpg', '3.jpg') 0 ('2.jpg', '4.jpg') 0 ('3.jpg', '4.jpg') 0 These two sets of pairs are then used to calculate metrics such as precision/recall/f1-score. It can be seen that the missing relationship between pair ('1jpg', '4.jpg') is accounted for only once.","title":"Considerations"},{"location":"user_guide/finding_duplicates/","text":"Finding duplicates There are two methods available to find duplicates: find_duplicates() find_duplicates_to_remove() find_duplicates() To find duplicates in an image directory, the general api is: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) Duplicates can also be found if encodings of the images are available: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( encoding_map , < threshold - parameter - value > ) where the returned variable duplicates is a dictionary with the following content: { 'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg': [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all file names in the image directory that were found to be duplicates for the key file. The 'method-name' corresponds to one of the deduplication methods available and can be set to: PHash AHash DHash WHash CNN Options image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding encodings (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. scores : Setting it to True returns the scores representing the hamming distance (for hashing) or cosine similarity (for cnn) of each of the duplicate file names from the key file. In this case, the returned 'duplicates' dictionary has the following content: { 'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of tuples representing the file names and corresponding scores in the image directory that were found to be duplicates of the key file. outfile : Name of file to which the returned duplicates dictionary is to be written, must be a json. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate of the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate of the key image. Should be an int between 0 and 64. Default value is 10. Considerations The returned duplicates dictionary contains symmetric relationships i.e., if an image i is a duplicate of image j , then image j must also be a duplicate of image i . Let's say that the image directory only consists of images i and j , then the duplicates dictionary would have the following content: { 'i': ['j'], 'j': ['i'] } If an image in the image directory can't be loaded, no encodings are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary. Examples To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, scores returned along with duplicate filenames and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , scores = True , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85, no scores returned and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , scores = False , outfile = 'my_duplicates.json' ) find_duplicates_to_remove() Returns a list of files in the image directory that are considered as duplicates. Does NOT remove the said files. The api is similar to find_duplicates function (except the score attribute in find_duplicates ). This function allows the return of a single list of file names in directory that are found to be duplicates. The general api for the method is as below: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) OR duplicates = method_object . find_duplicates_to_remove ( encoding_map = encoding_map , < threshold - parameter - value > ) In this case, the returned variable duplicates is a list containing the name of image files that are found to be duplicates of some file in the directory: [ 'image1_duplicate1.jpg', 'image1_duplicate2.jpg' ,.. ] The 'method-name' corresponds to one of the deduplication methods available and can be set to: PHash AHash DHash WHash CNN Options image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding encodings (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. outfile : Name of file to which the returned duplicates dictionary is to be written, must be a json. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10. Considerations This method must be used with caution. The symmetric nature of duplicates imposes an issue of marking one image as duplicate and the other as original. Consider the following duplicates dictionary: { '1.jpg': ['2.jpg'], '2.jpg': ['1.jpg', '3.jpg'], '3.jpg': ['2.jpg'] } In this case, it is possible to remove only 2.jpg which leaves 1.jpg and 3.jpg as non-duplicates of each other. However, it is also possible to remove both 1.jpg and 3.jpg leaving only 2.jpg . The find_duplicates_to_remove method can thus, return either of the outputs. In the above example, let's say that 1.jpg is retained, while its duplicate, 2.jpg , is marked as a duplicate. Once 2.jpg is marked as duplicate, its own found duplicates would be disregarded. Thus, 1.jpg and 3.jpg would not be considered as duplicates. So, the final return would be: ['2.jpg'] This leaves 1.jpg and 3.jpg as non-duplicates in the directory. If the user does not wish to impose this heuristic, it is advised to use find_duplicates function and use a custom heuristic to mark a file as duplicate. If an image in the image directory can't be loaded, no encodings are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary. Examples To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85 and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , outfile = 'my_duplicates.json' )","title":"Finding duplicates"},{"location":"user_guide/finding_duplicates/#finding-duplicates","text":"There are two methods available to find duplicates: find_duplicates() find_duplicates_to_remove()","title":"Finding duplicates"},{"location":"user_guide/finding_duplicates/#find_duplicates","text":"To find duplicates in an image directory, the general api is: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) Duplicates can also be found if encodings of the images are available: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( encoding_map , < threshold - parameter - value > ) where the returned variable duplicates is a dictionary with the following content: { 'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg': [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all file names in the image directory that were found to be duplicates for the key file. The 'method-name' corresponds to one of the deduplication methods available and can be set to: PHash AHash DHash WHash CNN","title":"find_duplicates()"},{"location":"user_guide/finding_duplicates/#options","text":"image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding encodings (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. scores : Setting it to True returns the scores representing the hamming distance (for hashing) or cosine similarity (for cnn) of each of the duplicate file names from the key file. In this case, the returned 'duplicates' dictionary has the following content: { 'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of tuples representing the file names and corresponding scores in the image directory that were found to be duplicates of the key file. outfile : Name of file to which the returned duplicates dictionary is to be written, must be a json. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate of the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate of the key image. Should be an int between 0 and 64. Default value is 10.","title":"Options"},{"location":"user_guide/finding_duplicates/#considerations","text":"The returned duplicates dictionary contains symmetric relationships i.e., if an image i is a duplicate of image j , then image j must also be a duplicate of image i . Let's say that the image directory only consists of images i and j , then the duplicates dictionary would have the following content: { 'i': ['j'], 'j': ['i'] } If an image in the image directory can't be loaded, no encodings are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary.","title":"Considerations"},{"location":"user_guide/finding_duplicates/#examples","text":"To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, scores returned along with duplicate filenames and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , scores = True , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85, no scores returned and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , scores = False , outfile = 'my_duplicates.json' )","title":"Examples"},{"location":"user_guide/finding_duplicates/#find_duplicates_to_remove","text":"Returns a list of files in the image directory that are considered as duplicates. Does NOT remove the said files. The api is similar to find_duplicates function (except the score attribute in find_duplicates ). This function allows the return of a single list of file names in directory that are found to be duplicates. The general api for the method is as below: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) OR duplicates = method_object . find_duplicates_to_remove ( encoding_map = encoding_map , < threshold - parameter - value > ) In this case, the returned variable duplicates is a list containing the name of image files that are found to be duplicates of some file in the directory: [ 'image1_duplicate1.jpg', 'image1_duplicate2.jpg' ,.. ] The 'method-name' corresponds to one of the deduplication methods available and can be set to: PHash AHash DHash WHash CNN","title":"find_duplicates_to_remove()"},{"location":"user_guide/finding_duplicates/#options_1","text":"image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding encodings (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. outfile : Name of file to which the returned duplicates dictionary is to be written, must be a json. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10.","title":"Options"},{"location":"user_guide/finding_duplicates/#considerations_1","text":"This method must be used with caution. The symmetric nature of duplicates imposes an issue of marking one image as duplicate and the other as original. Consider the following duplicates dictionary: { '1.jpg': ['2.jpg'], '2.jpg': ['1.jpg', '3.jpg'], '3.jpg': ['2.jpg'] } In this case, it is possible to remove only 2.jpg which leaves 1.jpg and 3.jpg as non-duplicates of each other. However, it is also possible to remove both 1.jpg and 3.jpg leaving only 2.jpg . The find_duplicates_to_remove method can thus, return either of the outputs. In the above example, let's say that 1.jpg is retained, while its duplicate, 2.jpg , is marked as a duplicate. Once 2.jpg is marked as duplicate, its own found duplicates would be disregarded. Thus, 1.jpg and 3.jpg would not be considered as duplicates. So, the final return would be: ['2.jpg'] This leaves 1.jpg and 3.jpg as non-duplicates in the directory. If the user does not wish to impose this heuristic, it is advised to use find_duplicates function and use a custom heuristic to mark a file as duplicate. If an image in the image directory can't be loaded, no encodings are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary.","title":"Considerations"},{"location":"user_guide/finding_duplicates/#examples_1","text":"To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85 and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , outfile = 'my_duplicates.json' )","title":"Examples"},{"location":"user_guide/plotting_duplicates/","text":"Plotting duplicates of an image Once a duplicate dictionary corresponding to an image directory has been obtained (using find_duplicates ), duplicates for an image can be plotted using plot_duplicates method as below: from imagededup.utils import plot_duplicates plot_duplicates ( image_dir , duplicate_map , filename , outfile = None ) where filename is the file for which duplicates are to be plotted. Options image_dir : Directory where all image files are present. duplicate_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. A duplicate_map with scores can also be passed (obtained from find_duplicates function with scores attribute set to True). filename : Image file name for which duplicates are to be plotted. outfile : Optional, name of the file the plot should be saved to. None by default. The output looks as below:","title":"Plotting duplicates"},{"location":"user_guide/plotting_duplicates/#plotting-duplicates-of-an-image","text":"Once a duplicate dictionary corresponding to an image directory has been obtained (using find_duplicates ), duplicates for an image can be plotted using plot_duplicates method as below: from imagededup.utils import plot_duplicates plot_duplicates ( image_dir , duplicate_map , filename , outfile = None ) where filename is the file for which duplicates are to be plotted.","title":"Plotting duplicates of an image"},{"location":"user_guide/plotting_duplicates/#options","text":"image_dir : Directory where all image files are present. duplicate_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. A duplicate_map with scores can also be passed (obtained from find_duplicates function with scores attribute set to True). filename : Image file name for which duplicates are to be plotted. outfile : Optional, name of the file the plot should be saved to. None by default. The output looks as below:","title":"Options"},{"location":"utils/data_generator/","text":"img_dataloader def img_dataloader ( image_dir , batch_size , basenet_preprocess , recursive ) class ImgDataset __init__ def __init__ ( image_dir , basenet_preprocess , recursive ) __len__ def __len__ () Number of images. __getitem__ def __getitem__ ( item ) class MobilenetV3 __init__ def __init__ () forward def forward ( x )","title":"Data generator"},{"location":"utils/data_generator/#img_dataloader","text":"def img_dataloader ( image_dir , batch_size , basenet_preprocess , recursive )","title":"img_dataloader"},{"location":"utils/data_generator/#class-imgdataset","text":"","title":"class ImgDataset"},{"location":"utils/data_generator/#__init__","text":"def __init__ ( image_dir , basenet_preprocess , recursive )","title":"__init__"},{"location":"utils/data_generator/#__len__","text":"def __len__ () Number of images.","title":"__len__"},{"location":"utils/data_generator/#__getitem__","text":"def __getitem__ ( item )","title":"__getitem__"},{"location":"utils/data_generator/#class-mobilenetv3","text":"","title":"class MobilenetV3"},{"location":"utils/data_generator/#__init___1","text":"def __init__ ()","title":"__init__"},{"location":"utils/data_generator/#forward","text":"def forward ( x )","title":"forward"},{"location":"utils/general_utils/","text":"get_files_to_remove def get_files_to_remove ( duplicates ) Get a list of files to remove. Args duplicates : A dictionary with file name as key and a list of duplicate file names as value. Returns save_json def save_json ( results , filename , float_scores ) Save results with a filename. Args results : Dictionary of results to be saved. filename : Name of the file to be saved. float_scores : boolean to indicate if scores are floats. parallelise def parallelise ( function , data , verbose ) generate_files def generate_files ( image_dir , recursive ) generate_relative_names def generate_relative_names ( image_dir , files )","title":"General utils"},{"location":"utils/general_utils/#get_files_to_remove","text":"def get_files_to_remove ( duplicates ) Get a list of files to remove.","title":"get_files_to_remove"},{"location":"utils/general_utils/#args","text":"duplicates : A dictionary with file name as key and a list of duplicate file names as value.","title":"Args"},{"location":"utils/general_utils/#returns","text":"","title":"Returns"},{"location":"utils/general_utils/#save_json","text":"def save_json ( results , filename , float_scores ) Save results with a filename.","title":"save_json"},{"location":"utils/general_utils/#args_1","text":"results : Dictionary of results to be saved. filename : Name of the file to be saved. float_scores : boolean to indicate if scores are floats.","title":"Args"},{"location":"utils/general_utils/#parallelise","text":"def parallelise ( function , data , verbose )","title":"parallelise"},{"location":"utils/general_utils/#generate_files","text":"def generate_files ( image_dir , recursive )","title":"generate_files"},{"location":"utils/general_utils/#generate_relative_names","text":"def generate_relative_names ( image_dir , files )","title":"generate_relative_names"},{"location":"utils/image_utils/","text":"check_image_array_hash def check_image_array_hash ( image_arr ) Checks the sanity of the input image numpy array for hashing functions. Args image_arr : Image array. expand_image_array_cnn def expand_image_array_cnn ( image_arr ) Checks the sanity of the input image numpy array for cnn and converts the grayscale numpy array to rgb by repeating the array thrice along the 3rd dimension if a 2-dimensional image array is provided. Args image_arr : Image array. Returns preprocess_image def preprocess_image ( image , target_size , grayscale ) Take as input an image as numpy array or Pillow format. Returns an array version of optionally resized and grayed image. Args image : numpy array or a pillow image. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image. Returns load_image def load_image ( image_file , target_size , grayscale , img_formats ) Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images of types described by img_formats argument. Args image_file : Path to the image file. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image. img_formats : List of allowed image formats that can be loaded.","title":"Image utils"},{"location":"utils/image_utils/#check_image_array_hash","text":"def check_image_array_hash ( image_arr ) Checks the sanity of the input image numpy array for hashing functions.","title":"check_image_array_hash"},{"location":"utils/image_utils/#args","text":"image_arr : Image array.","title":"Args"},{"location":"utils/image_utils/#expand_image_array_cnn","text":"def expand_image_array_cnn ( image_arr ) Checks the sanity of the input image numpy array for cnn and converts the grayscale numpy array to rgb by repeating the array thrice along the 3rd dimension if a 2-dimensional image array is provided.","title":"expand_image_array_cnn"},{"location":"utils/image_utils/#args_1","text":"image_arr : Image array.","title":"Args"},{"location":"utils/image_utils/#returns","text":"","title":"Returns"},{"location":"utils/image_utils/#preprocess_image","text":"def preprocess_image ( image , target_size , grayscale ) Take as input an image as numpy array or Pillow format. Returns an array version of optionally resized and grayed image.","title":"preprocess_image"},{"location":"utils/image_utils/#args_2","text":"image : numpy array or a pillow image. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image.","title":"Args"},{"location":"utils/image_utils/#returns_1","text":"","title":"Returns"},{"location":"utils/image_utils/#load_image","text":"def load_image ( image_file , target_size , grayscale , img_formats ) Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images of types described by img_formats argument.","title":"load_image"},{"location":"utils/image_utils/#args_3","text":"image_file : Path to the image file. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image. img_formats : List of allowed image formats that can be loaded.","title":"Args"},{"location":"utils/logger/","text":"return_logger def return_logger ( name )","title":"Logger"},{"location":"utils/logger/#return_logger","text":"def return_logger ( name )","title":"return_logger"},{"location":"utils/plotter/","text":"plot_duplicates def plot_duplicates ( image_dir , duplicate_map , filename , outfile ) Given filename for an image, plot duplicates along with the original image using the duplicate map obtained using find_duplicates method. Args image_dir : image directory where all files in duplicate_map are present. duplicate_map : mapping of filename to found duplicates (could be with or without scores). filename : Name of the file for which duplicates are to be plotted, must be a key in the duplicate_map. outfile : Optional, name of the file to save the plot. Default is None. Example usage from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , duplicate_map = duplicate_map , filename = 'path/to/image.jpg' )","title":"Plot duplicates"},{"location":"utils/plotter/#plot_duplicates","text":"def plot_duplicates ( image_dir , duplicate_map , filename , outfile ) Given filename for an image, plot duplicates along with the original image using the duplicate map obtained using find_duplicates method.","title":"plot_duplicates"},{"location":"utils/plotter/#args","text":"image_dir : image directory where all files in duplicate_map are present. duplicate_map : mapping of filename to found duplicates (could be with or without scores). filename : Name of the file for which duplicates are to be plotted, must be a key in the duplicate_map. outfile : Optional, name of the file to save the plot. Default is None.","title":"Args"},{"location":"utils/plotter/#example-usage","text":"from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , duplicate_map = duplicate_map , filename = 'path/to/image.jpg' )","title":"Example usage"}]}